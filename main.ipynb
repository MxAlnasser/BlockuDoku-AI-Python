{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99zGdQOOs22N"
   },
   "source": [
    "# Machine Learning Final Project: Blockudoku Double Deep Convolutional Q Learning Artificial Neural Network\n",
    "###### (the more words I put, the cooler it sounds)\n",
    "## Group 1, Bot Bot\n",
    "## Group Members: Mohammed Alnasser, Jesus Nu Ìƒnez, Ankit Dhingra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EvREuzKs22T"
   },
   "source": [
    "In 2016, Google DeepMind ([Link](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847)) decided to alter the DQN algorithm the same way the original Q-Learner algorithm was updated by adding a second network. The team found that the overestimation that affected the q-learner also affected the DQN algoritm. When the original double q-learner was introduced they proved that it worked in that setting and in this paper they prove it can be generalized to work with large scale function approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRoGB1f_s22U"
   },
   "source": [
    "**Summary**  \n",
    "This algorithm combines the benefits of the Double Q-Learner as the benefits of deep learning. We get the function approximation so that we can have a continuous state space plus we keep from having the overestimation from Q-Learning. This document will be a combination of the DQN and Double Q-Learner so it will be mostly review. But, this algorithm is so much more powerful that you should explore the other gyms at OpenAI and see what you can solve.  \n",
    "\n",
    "One thing to note, we need to update the weights of the second neural network with the weights from the first neural network. We didn't do this in the Double Q-Learner since the tables were both getting updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvOKe8TQs22V"
   },
   "source": [
    "**CartPole Example**  \n",
    "Again we will use the [CartPole](https://gym.openai.com/envs/CartPole-v1/) environment from OpenAI.  \n",
    "\n",
    "The actions are 0 to push the cart to the left and 1 to push the cart to the right.  \n",
    "\n",
    "The continuous state space is an X coordinate for location, the velocity of the cart, the angle of the pole, and the velocity at the tip of the pole. The X coordinate goes from -4.8 to +4.8, velocity is -Inf to +Inf, angle of the pole goes from -24 degrees to +24 degrees, tip velocity is -Inf to +Inf. With all of the possible combinations you can see why we can't create a Q table for each one.  \n",
    "\n",
    "To \"solve\" this puzzle you have to have an average reward of > 195 over 100 consecutive episodes. One thing to note, I am hard capping the rewards at 210 so this number can't average above that and it also could potentially drive the average down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0Sk6Yyns22W",
    "outputId": "c2126d64-2961-4d4e-e516-596c38dd8b77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.2 (SDL 2.0.16, Python 3.9.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from Engine import Blockudoku\n",
    "import pygame as pg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "#Create game\n",
    "game = Blockudoku(69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IxDvC7ls22Z"
   },
   "source": [
    "**Double Deep Q-Network Class**  \n",
    "This class is the same as the DQN class from the last notebook with a few exceptions.  \n",
    "**init**:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We create a second NN for the target network  \n",
    "\n",
    "**update_target_from_model(self)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This class updates the weights of the target NN from the model NN\n",
    "\n",
    "**build_model(self)**:  \n",
    "**action(self,state)**:  \n",
    "**test_action(self,state)**:  \n",
    "**store(self, state, action, reward, nstate, done)**:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Same  \n",
    "\n",
    "**experience_replay(self, batch_size)**:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This class has the Double DQN changes. We grab the prediction targets from the target NN and then use that in the Q update rule.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BgVQxQy6s22Z"
   },
   "outputs": [],
   "source": [
    "class DoubleDeepQNetwork():\n",
    "    def __init__(self, states, actions, alpha, gamma, epsilon,epsilon_min, epsilon_decay):\n",
    "        self.nS = states\n",
    "        self.nA = actions\n",
    "        self.memory = deque([], maxlen=2500)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        #Explore/Exploit\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.model = self.build_model()\n",
    "        self.model_target = self.build_model() #Second (target) neural network\n",
    "        self.update_target_from_model() #Update weights\n",
    "        self.loss = []\n",
    "        \n",
    "    def build_model(self):\n",
    "        cnn_layers = [Input(shape=self.nS),\n",
    "                      Conv2D(16, 3, activation=\"relu\", padding=\"same\" , name=\"Conv2D_layer1\"),\n",
    "                      MaxPool2D(),\n",
    "                      Conv2D(32, 3, activation=\"relu\", padding=\"same\", name=\"Conv2D_layer2\"),\n",
    "                      MaxPool2D(),\n",
    "                      Dense(69, activation=\"relu\", name=\"Dense_layer1\"),\n",
    "                      Dense(69, activation=\"relu\", name=\"Dense_layer2\"),\n",
    "                      Flatten(),\n",
    "                      \n",
    "                      Dense(self.nA, activation=\"linear\", name=\"output\")]\n",
    "        model = keras.Sequential(cnn_layers)\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=self.alpha))\n",
    "        return model\n",
    "\n",
    "    def update_target_from_model(self):\n",
    "        #Update the target model from the base model\n",
    "        self.model_target.set_weights( self.model.get_weights() )\n",
    "\n",
    "    def action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.nA) #Explore\n",
    "        action_vals = self.model.predict(state) #Exploit: Use the NN to predict the correct action from this state\n",
    "        return np.argmax(action_vals[0])\n",
    "\n",
    "    def test_action(self, state): #Exploit\n",
    "        action_vals = self.model.predict(state)\n",
    "        return np.argmax(action_vals[0])\n",
    "\n",
    "    def store(self, state, action, reward, nstate, done):\n",
    "        #Store the experience in memory\n",
    "        self.memory.append( (state, action, reward, nstate, done) )\n",
    "\n",
    "    def experience_replay(self, batch_size):\n",
    "        #Execute the experience replay\n",
    "        minibatch = random.sample( self.memory, batch_size) #Randomly sample from memory\n",
    "\n",
    "        #Convert to numpy for speed by vectorization\n",
    "        x = []\n",
    "        y = []\n",
    "        np_array = np.array(minibatch)\n",
    "        st = np.zeros((0, self.nS[0], self.nS[1], self.nS[2])) #States\n",
    "        nst = np.zeros( (0, self.nS[0], self.nS[1], self.nS[2]) )#Next States\n",
    "        for i in range(len(np_array)): #Creating the state and next state np arrays\n",
    "            st = np.append( st, np_array[i,0], axis=0)\n",
    "            nst = np.append( nst, np_array[i,3], axis=0)\n",
    "        st_predict = self.model.predict(st) #Here is the speedup! I can predict on the ENTIRE batch\n",
    "        nst_predict = self.model.predict(nst)\n",
    "        nst_predict_target = self.model_target.predict(nst) #Predict from the TARGET\n",
    "        index = 0\n",
    "        for state, action, reward, nstate, done in minibatch:\n",
    "            x.append(state)\n",
    "            #Predict from state\n",
    "            nst_action_predict_target = nst_predict_target[index]\n",
    "            nst_action_predict_model = nst_predict[index]\n",
    "            if done == True: #Terminal: Just assign reward much like {* (not done) - QB[state][action]}\n",
    "                target = reward\n",
    "            else:   #Non terminal\n",
    "                target = reward + self.gamma * nst_action_predict_target[np.argmax(nst_action_predict_model)] \n",
    "            target_f = st_predict[index]\n",
    "            target_f[action] = target\n",
    "            y.append(target_f)\n",
    "            index += 1\n",
    "        #Reshape for Keras Fit\n",
    "        x_reshape = np.array(x).reshape((batch_size, self.nS[0], self.nS[1], self.nS[2]))\n",
    "        y_reshape = np.array(y).reshape((batch_size, self.nA))\n",
    "        epoch_count = 1\n",
    "        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n",
    "        #Graph Losses\n",
    "        for i in range(epoch_count):\n",
    "            self.loss.append( hist.history['loss'][i] )\n",
    "        #Decay Epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bhcyQKRks22b"
   },
   "outputs": [],
   "source": [
    "#Create the agents\n",
    "EPISODES = 200\n",
    "TRAIN_END = 0\n",
    "discount_rate = 0.3  # Gamma\n",
    "learning_rate = 0.001  # Alpha\n",
    "batch_size = 24\n",
    "\n",
    "nS = game.state.shape\n",
    "nA = game.n_actions #Actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DoubleDeepQNetwork(nS, nA, learning_rate, discount_rate, 1, 0.001, 0.99995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCJSobFls22c",
    "outputId": "8421439c-a9fe-400e-b288-d88e2298b60a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-cbf1757a7adc>:58: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np_array = np.array(minibatch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/200, score: 16, actions: 100, e: 0.9962071162205131\n",
      "episode: 1/200, score: 17, actions: 103, e: 0.9910897102669042\n",
      "episode: 2/200, score: 31, actions: 114, e: 0.9854564281915569\n",
      "episode: 3/200, score: 15, actions: 99, e: 0.9805903506970227\n",
      "episode: 4/200, score: 25, actions: 114, e: 0.9750167462204384\n",
      "episode: 5/200, score: 17, actions: 107, e: 0.9698142057687201\n",
      "episode: 6/200, score: 46, actions: 114, e: 0.9643018521188138\n",
      "episode: 7/200, score: 19, actions: 110, e: 0.9590126184264745\n",
      "episode: 8/200, score: 13, actions: 95, e: 0.9544679968936504\n",
      "episode: 9/200, score: 15, actions: 104, e: 0.9495175219371295\n",
      "episode: 10/200, score: 27, actions: 106, e: 0.9444982663652228\n",
      "episode: 11/200, score: 17, actions: 102, e: 0.9396934677369528\n",
      "episode: 12/200, score: 24, actions: 109, e: 0.9345859413006028\n",
      "episode: 13/200, score: 10, actions: 92, e: 0.9302966117581403\n",
      "episode: 14/200, score: 15, actions: 94, e: 0.9259343679292322\n",
      "episode: 15/200, score: 16, actions: 102, e: 0.9212240065243188\n",
      "episode: 16/200, score: 23, actions: 102, e: 0.9165376074058627\n",
      "episode: 17/200, score: 24, actions: 102, e: 0.9118750486742638\n",
      "episode: 18/200, score: 22, actions: 102, e: 0.9072362090500423\n",
      "episode: 19/200, score: 16, actions: 99, e: 0.9027563745563085\n",
      "episode: 20/200, score: 13, actions: 95, e: 0.8984783431937656\n",
      "episode: 21/200, score: 19, actions: 105, e: 0.8937735750946064\n",
      "episode: 22/200, score: 14, actions: 91, e: 0.8897160417773075\n",
      "episode: 23/200, score: 19, actions: 99, e: 0.8853227199788358\n",
      "episode: 24/200, score: 15, actions: 95, e: 0.8811273041736549\n",
      "episode: 25/200, score: 21, actions: 108, e: 0.8763819221585804\n",
      "episode: 26/200, score: 18, actions: 95, e: 0.8722288754959046\n",
      "episode: 27/200, score: 40, actions: 114, e: 0.8672711897899273\n",
      "episode: 28/200, score: 17, actions: 107, e: 0.8626435631722484\n",
      "episode: 29/200, score: 23, actions: 107, e: 0.858040628863463\n",
      "episode: 30/200, score: 21, actions: 110, e: 0.8533342421717759\n",
      "episode: 31/200, score: 17, actions: 97, e: 0.8492054881993341\n",
      "episode: 32/200, score: 14, actions: 99, e: 0.845012203142638\n",
      "episode: 33/200, score: 17, actions: 99, e: 0.8408396240750231\n",
      "episode: 34/200, score: 13, actions: 105, e: 0.8364366738316195\n",
      "episode: 35/200, score: 15, actions: 91, e: 0.8326394372982803\n",
      "episode: 36/200, score: 16, actions: 103, e: 0.8283622604499506\n",
      "episode: 37/200, score: 12, actions: 82, e: 0.8249728434988516\n",
      "episode: 38/200, score: 30, actions: 118, e: 0.8201197132057712\n",
      "episode: 39/200, score: 13, actions: 95, e: 0.8162332849810366\n",
      "episode: 40/200, score: 9, actions: 87, e: 0.8126902932099322\n",
      "episode: 41/200, score: 14, actions: 96, e: 0.8087986299740766\n",
      "episode: 42/200, score: 9, actions: 100, e: 0.8047646293792129\n",
      "episode: 43/200, score: 18, actions: 101, e: 0.800710711410557\n",
      "episode: 44/200, score: 18, actions: 100, e: 0.7967170504837972\n",
      "episode: 45/200, score: 14, actions: 96, e: 0.7929018770030104\n"
     ]
    }
   ],
   "source": [
    "# start pygame\n",
    "pg.init()\n",
    "screen = pg.display.set_mode([game.window_size.x, game.window_size.y])\n",
    "game.reset()\n",
    "\n",
    "#Training\n",
    "rewards = [] #Store rewards for graphing\n",
    "epsilons = [] # Store the Explore/Exploit\n",
    "TEST_Episodes = 0\n",
    "running = True\n",
    "for e in range(EPISODES):\n",
    "    state = game.reset()\n",
    "    state = np.reshape(state, [1, nS[0], nS[1], nS[2]]) # Resize to store in memory to pass to .predict\n",
    "    tot_rewards = 0\n",
    "    time = 0\n",
    "    while True: #200 is when you \"solve\" the game. This can continue forever as far as I know\n",
    "        running = game.drawGameHeadless(screen)\n",
    "        \n",
    "        if not running:\n",
    "            break\n",
    "        \n",
    "        action = dqn.action(state)\n",
    "        nstate, reward, done = game.step(action)\n",
    "        nstate = np.reshape(state, [1, nS[0], nS[1], nS[2]])\n",
    "        tot_rewards += reward\n",
    "        dqn.store(state, action, reward, nstate, done) # Resize to store in memory to pass to .predict\n",
    "        state = nstate\n",
    "        \n",
    "        if done or tot_rewards < -1000:\n",
    "            rewards.append(tot_rewards)\n",
    "            epsilons.append(dqn.epsilon)\n",
    "            print(\"episode: {}/{}, score: {}, actions: {}, e: {}\"\n",
    "                  .format(e, EPISODES, game.score, time, dqn.epsilon))\n",
    "            break\n",
    "        #Experience Replay\n",
    "        if len(dqn.memory) > batch_size:\n",
    "            dqn.experience_replay(batch_size)\n",
    "        \n",
    "        time += 1\n",
    "            \n",
    "    if not running:\n",
    "        break\n",
    "    #Update the weights after each episode (You can configure this for x steps as well\n",
    "    dqn.update_target_from_model()\n",
    "    #If our current NN passes we are done\n",
    "    #I am going to use the last 5 runs\n",
    "    if len(rewards) > 5 and np.average(rewards[-5:]) > 195:\n",
    "        #Set the rest of the EPISODES for testing\n",
    "        TEST_Episodes = EPISODES - e\n",
    "        TRAIN_END = e\n",
    "        break\n",
    "\n",
    "pg.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vny_JMOs22d",
    "outputId": "53e39eab-b287-4505-b083-cd51d13365ca"
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "print('Training complete. Testing started...')\n",
    "#TEST Time\n",
    "#   In this section we ALWAYS use exploit don't train any more\n",
    "for e_test in range(TEST_Episodes):\n",
    "    state = envCartPole.reset()\n",
    "    state = np.reshape(state, [1, nS])\n",
    "    tot_rewards = 0\n",
    "    for t_test in range(210):\n",
    "        action = dqn.test_action(state)\n",
    "        nstate, reward, done, _ = envCartPole.step(action)\n",
    "        nstate = np.reshape( nstate, [1, nS])\n",
    "        tot_rewards += reward\n",
    "        #DON'T STORE ANYTHING DURING TESTING\n",
    "        state = nstate\n",
    "        if done or t_test == 209: \n",
    "            rewards.append(tot_rewards)\n",
    "            epsilons.append(0) #We are doing full exploit\n",
    "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
    "                  .format(e_test, TEST_Episodes, tot_rewards, 0))\n",
    "            break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk6TQqyLs22d"
   },
   "source": [
    "**Results**  \n",
    "Here is a graph of the results. If everything was done correctly you should see the rewards over the red line.  \n",
    "\n",
    "Black: This is the 100 episode rolling average  \n",
    "Red: This is the \"solved\" line at 195  \n",
    "Blue: This is the reward for each episode  \n",
    "Green: This is the value of epsilon scaled by 200  \n",
    "Yellow: This is where the tests started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCmmo-1-s22e",
    "outputId": "e4a8344d-b461-4734-f314-d504937f40f2"
   },
   "outputs": [],
   "source": [
    "#Plotting\n",
    "rolling_average = np.convolve(rewards, np.ones(100)/100)\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(rolling_average, color='black')\n",
    "plt.axhline(y=195, color='r', linestyle='-') #Solved Line\n",
    "#Scale Epsilon (0.001 - 1.0) to match reward (0 - 200) range\n",
    "eps_graph = [200*x for x in epsilons]\n",
    "plt.plot(eps_graph, color='g', linestyle='-')\n",
    "#Plot the line where TESTING begins\n",
    "plt.axvline(x=TRAIN_END, color='y', linestyle='-')\n",
    "plt.xlim( (0,EPISODES) )\n",
    "plt.ylim( (0,220) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfgEDnPHs22f"
   },
   "source": [
    "**Changes**  \n",
    "These are all the same changes as the DQN notebook with the exception of the update weights parameter.  \n",
    "*hyper parameters*: You can alter alpha, gamma, batch size, and episode length to see what differences the algorithm returns.  \n",
    "*Training End*: You can also change the line where I only check the last 5 runs before switching to testing mode (if len(rewards) > 5 and np.average(rewards[-5:]) > 195:) as that doesn't prove it was solved. The reason I did this was because I wanted to limit the amount of runs I made.  \n",
    "*Update Weights*: I call 'dqn.update_target_from_model()' after every episode. You can adjust this to run at different times. I have done per step (no matter how long the episode ran) and I have seen it done every X episodes. Feel free to try different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TckSaJis22f"
   },
   "source": [
    "**Conclusion**  \n",
    "This is a Double Deep Q-Network implementation. There are some changes you can make here and there but it follows the paper as close as I could. If you want to dive deeper you can see that the paper has graphs that dive deeper into the inner workings of the neural network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_ZOzYeMs22g"
   },
   "source": [
    "**Reference**  \n",
    "Van Hasselt, H., Guez, A., & Silver, D. (2016, February). Deep Reinforcement Learning with Double Q-Learning. In AAAI (Vol. 2, p. 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNKLFVAws22g"
   },
   "source": [
    "## Project Conclusion  \n",
    "This completes the set of notebooks that cover the original Q-Learner and continues through recent updates. Once you have worked in this area for a while you can see how powerful that first update statement really was. With just some slight tweaks, these algorithms were able to achieve higher scores than even advanced Atari players.  \n",
    "\n",
    "I hope these notebooks have peaked your interest enough to continue your reinforcement journey."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "06-DDQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
